- The concept of converting data into a vector format is often referred to as embedding.

- While word embeddings are the most common form of text embedding, there are also embeddings for sentences,
  paragraphs, or whole documents. Sentence or paragraph embeddings are popular choices for retrieval-augmented
  generation. Retrieval-augmented generation combines generation (like producing text) with retrieval 
  like searching an external knowledge base) to pull relevant information when generating text,

- Word embeddings can have varying dimensions, from one to thousands. A higher dimensionality might 
  capture more nuanced relationships but at the cost of computational efficiency.

- The smallest GPT-2 models (117M and 125M parameters) use an embedding size of 768 dimensions to provide
  concrete examples. The largest GPT-3 model (175B parameters) uses an embedding size of 12,288 dimensions.


- Depending on the LLM, some researchers also consider additional special tokens such as the following:
  [BOS] (beginning of sequence)—This token marks the start of a text. It signifies to
   the LLM where a piece of content begins.
  [EOS] (end of sequence)—This token is positioned at the end of a text and is especially useful when
   concatenating multiple unrelated texts, similar to <|endoftext|>. For instance, when combining two 
   different Wikipedia arti- cles or books, the [EOS] token indicates where one ends and the next begins.
  [PAD] (padding)—When training LLMs with batch sizes larger than one, the batch might contain texts 
   of varying lengths. To ensure all texts have the same length, the shorter texts are extended or 
   “padded” using the [PAD] token, up to the length of the longest text in the batch.

- Moreover, the tokenizer used for GPT models also doesn’t use an <|unk|> token for out-of-vocabulary words.
  Instead, GPT models use a byte pair encoding tokenizer, which breaks words down into subword units,

- byte pair encoding (BPE). The BPE tokenizer was used to train LLMs such as GPT-2, GPT-3,
  and the original model used in ChatGPT. the BPE tokenizer has a total vocabulary size of 50,257, 
  with <|endoftext|> being assigned the largest token ID.

- The algorithm underlying BPE breaks down words that aren’t in its predefined vocabulary into smaller
  subword units or even individual characters, enabling it to handle out-of-vocabulary words. So, thanks
  to the BPE algorithm, if the tokenizer encounters an unfamiliar word during tokenization, it can 
  represent it as a sequence of subword tokens or characters 

- A detailed discussion and implementation of BPE is out of the scope of this book, but in short, it builds
  its vocabulary by iteratively merging frequent characters into sub- words and frequent subwords into 
  words. For example, BPE starts with adding all individual single characters to its vocabulary 
  (“a,” “b,” etc.). In the next stage, it merges character combinations that frequently occur together 
  into subwords. For example, “d” and “e” may be merged into the subword “de,” which is common in many 
  English words like “define,” “depend,” “made,” and “hidden.” The merges are determined by a frequency cutoff.

- One of the easiest and most intuitive ways to create the input–target pairs for the next-word 
  prediction task is to create two variables, x and y, where x contains the input tokens and y 
  contains the targets, which are the inputs shifted by 1

- NOTE
  For those who are familiar with one-hot encoding, the embedding layer approach described here is 
  essentially just a more efficient way of implementing one-hot encoding followed by matrix multiplication
  in a fully connected layer, which is illustrated in the supplementary code on GitHub at https://mng.bz/ZEB5.
  Because the embedding layer is just a more efficient implementation equivalent to the one-hot encoding
  and matrix-multiplication approach, it can be seen as a neural network layer that can be optimized
  via backpropagation.

- In principle, the deterministic, position-independent embedding of the token ID is good for reproducibility
  purposes. However, since the self-attention mechanism of LLMs itself is also position-agnostic, it is 
  helpful to inject additional position information into the LLM. To achieve this, we can use two broad 
  categories of position-aware embeddings: relative positional embeddings and absolute positional embeddings.
  - Absolute positional embeddings are directly associated with specific positions in a sequence. For each 
    position in the input sequence, a unique embedding is added to the token’s embedding to convey its exact
    location. For instance, the first token will have a specific positional embedding, the second token 
    another distinct embedding, and so on, as illustrated in figure 2.18. 
  - Instead of focusing on the absolute position of a token, the emphasis of relative positional 
    embeddings is on the relative position or distance between tokens. This means the model learns the
    relationships in terms of “how far apart” rather than “at which exact position.” The advantage here 
    is that the model can generalize better to sequences of varying lengths, even if it hasn’t seen 
    such lengths during training.
  
- The context_length is a variable that represents the supported input size of the LLM. Here, we choose
  it similar to the maximum length of the input text. In practice, input text can be longer than the 
  supported context length, in which case we have to truncate the text.

Summary
  LLMs require textual data to be converted into numerical vectors, known as embeddings, since they can’t
   process raw text. Embeddings transform discrete data (like words or images) into continuous vector 
   spaces, making them compatible with neural network operations.
  As the first step, raw text is broken into tokens, which can be words or characters. Then, the 
   tokens are converted into integer representations, termed token IDs.
  Special tokens, such as <|unk|> and <|endoftext|>, can be added to enhance the model’s understanding
   and handle various contexts, such as unknown words or marking the boundary between unrelated texts.
  The byte pair encoding (BPE) tokenizer used for LLMs like GPT-2 and GPT-3 can efficiently handle 
   unknown words by breaking them down into subword units or individual characters.
  We use a sliding window approach on tokenized data to generate input–target pairs for LLM training.
  Embedding layers in PyTorch function as a lookup operation, retrieving vectors corresponding to token
   IDs. The resulting embedding vectors provide continuous representations of tokens, which is crucial
   for training deep learning models like LLMs.
  While token embeddings provide consistent vector representations for each token, they lack a sense
   of the token’s position in a sequence. To rectify this, two main types of positional embeddings exist:
   absolute and relative. OpenAI’s GPT models utilize absolute positional embeddings, which are added 
   to the token embedding vectors and are optimized during the model training.


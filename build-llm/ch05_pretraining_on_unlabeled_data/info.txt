
- Weight parameters
  In the context of LLMs and other deep learning models, weights refer to the trainable parameters that
  the learning process adjusts. These weights are also known as weight parameters or simply parameters.
  In frameworks like PyTorch, these weights are stored in linear layers; we used these to implement the 
  multi-head attention module in chapter 3 and the GPTModel in chapter 4. After initializing a layer 
  (new_layer = torch.nn.Linear(...)), we can access its weights through the .weight attribute, 
  new_layer.weight. Additionally, for convenience, PyTorch allows direct access to all a model’s 
  trainable parameters, including weights and biases, through the method model.parameters(),
  
- Originally, the GPT-2 model with 124 million parameters was configured to handle up to 1,024 tokens.
  After the training process, we will update the context size setting and load pretrained weights to 
  work with a model configured for a 1,024-token context length.

- Remember that figure 5.6 displays the softmax probabilities for a compact seven-token vocabulary to 
  fit everything into a single figure. This implies that the starting random values will hover around
  1/7, which equals approximately 0.14. However, the vocabulary we are using for our GPT-2 model has 
  50,257 tokens, so most of the initial probabilities will hover around 0.00002 (1/50,257).

- Backpropagation
  How do we maximize the softmax probability values corresponding to the target tokens? The big picture
  is that we update the model weights so that the model outputs higher values for the respective token 
  IDs we want to generate. The weight update is done via a process called backpropagation, a standard 
  technique for training deep neural networks (see sections A.3 to A.7 in appendix A for more details 
  about backpropagation and model training). Backpropagation requires a loss function, which calculates 
  the difference between the model’s predicted output (here, the probabilities corresponding to the 
  target token IDs) and the actual desired output. This loss function measures how far off the model’s 
  predictions are from the target values.

- In deep learning, the term for turning this negative value, –10.7940, into 10.7940, is known as the
  cross entropy loss. PyTorch comes in handy here, as it already has a built-in cross_entropy function
  that takes care of all these six steps in figure 5.7 for us.

- Cross entropy loss
  At its core, the cross entropy loss is a popular measure in machine learning and deep learning that
  measures the difference between two probability distributions—typically, the true distribution of 
  labels (here, tokens in a dataset) and the predicted distribution from a model (for instance, the token 
  probabilities generated by an LLM). In the context of machine learning and specifically in frameworks 
  like PyTorch, the cross_entropy function computes this measure for discrete outcomes, which is similar
  to the negative average log probability of the target tokens given the model’s generated token 
  probabilities, making the terms “cross entropy” and “negative aver- age log probability” related 
  and often used interchangeably in practice.

- Perplexity is a measure often used alongside cross entropy loss to evaluate the performance of models
  in tasks like language modeling. It can provide a more interpretable way to understand the uncertainty
  of a model in predicting the next token in a sequence. Perplexity measures how well the probability 
  distribution predicted by the model matches the actual distribution of the words in the dataset. Similar
  to the loss, a lower perplexity indicates that the model predictions are closer to the actual distribution.
  Perplexity can be calculated as perplexity = torch.exp(loss), which returns tensor(48725.8203) when 
  applied to the previously calculated loss. Perplexity is often considered more interpretable than the 
  raw loss value because it signifies the effective vocabulary size about which the model is uncertain 
  at each step. In the given example, this would translate to the model being unsure about which among
  48,725 tokens in the vocabulary to generate as the next token.
  
- The cost of pretraining LLMs
  To put the scale of our project into perspective, consider the training of the 7 billion parameter 
  Llama 2 model, a relatively popular openly available LLM. This model required 184,320 GPU hours on
  expensive A100 GPUs, processing 2 trillion tokens. At the time of writing, running an 8 × A100 cloud
  server on AWS costs around $30 per hour. A rough estimate puts the total training cost of such an LLM
  at around $690,000 (calculated as 184,320 hours divided by 8, then multiplied by $30).

- NOTE
  We are training the model with training data presented in similarly sized chunks for simplicity and 
  efficiency. However, in practice, it can also be beneficial to train an LLM with variable-length 
  inputs to help the LLM to better generalize across different types of inputs when it is being used.

- We used a relatively small batch size to reduce the computational resource demand because we were working
  with a very small dataset. In practice, training LLMs with batch sizes of 1,024 or larger is not uncommon.

- AdamW
  Adam optimizers are a popular choice for training deep neural networks. However, in our training loop, we opt
  for the AdamW optimizer. AdamW is a variant of Adam that improves the weight decay approach, which aims to 
  minimize model complexity and prevent overfitting by penalizing larger weights. This adjustment allows AdamW to
  achieve more effective regularization and better generalization; thus, AdamW is frequently used in the training
  of LLMs.

- If we replaced the argmax function with the multinomial function inside the generate_and_print_sample function,
  the LLM would sometimes generate texts such as every effort moves you toward, every effort moves you inches , 
  and every effort moves you closer instead of every effort moves you forward 

- Top-k sampling, when combined with probabilistic sampling and temperature scaling, can improve the text 
  generation results. In top-k sampling, we can restrict the sampled tokens to the top-k most likely 
  tokens and exclude all other tokens from the selection process by masking their probability scores, 
  as illustrated in figure 5.15.

- Adaptive optimizers such as AdamW store additional parameters for each model weight. AdamW uses historical
  data to adjust learning rates for each model parameter dynamically. Without it, the optimizer resets, and 
  the model may learn suboptimally or even fail to converge properly, which means it will lose the ability to
  generate coherent text. Using torch.save, we can save both the model and optimizer state_dict contents


Summary
  When LLMs generate text, they output one token at a time.
  By default, the next token is generated by converting the model outputs into probability scores
   and selecting the token from the vocabulary that corresponds to the highest probability score, 
   which is known as “greedy decoding.”
  Using probabilistic sampling and temperature scaling, we can influence the diversity and coherence
   of the generated text.
  Training and validation set losses can be used to gauge the quality of text 
   generated by LLM during training.
  Pretraining an LLM involves changing its weights to minimize the training loss.
  The training loop for LLMs itself is a standard procedure in deep learning,
   using a conventional cross entropy loss and AdamW optimizer.
  Pretraining an LLM on a large text corpus is time- and resource-intensive, so we can load 
   openly available weights as an alternative to pretraining the model on a large dataset ourselves.

